{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeremygoldwasser/opt/anaconda3/envs/shap/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"../../HelperFiles\")\n",
    "from helper import *\n",
    "from shapley_sampling2 import *\n",
    "from train_models import *\n",
    "from load_data import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m xloc \u001b[38;5;241m=\u001b[39m X_test[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# model, approx, true_shap_vals = train_logreg(X_train, y_train, xloc, mapping_dict)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model, approx, true_shap_vals \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_neural_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass imbalance: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mround\u001b[39m(\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mmax\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(y_test), \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mmean(y_test))))))\n\u001b[1;32m      7\u001b[0m Y_preds \u001b[38;5;241m=\u001b[39m (model(X_test) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/RankSHAP/Experiments/Notebooks/../../HelperFiles/train_models.py:60\u001b[0m, in \u001b[0;36mtrain_neural_net\u001b[0;34m(X_train, y_train, xloc, mapping_dict)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/jeremygoldwasser/Desktop/RankSHAP/Experiments/Notebooks/../../HelperFiles/train_models.py?line=57'>58</a>\u001b[0m         loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     <a href='file:///Users/jeremygoldwasser/Desktop/RankSHAP/Experiments/Notebooks/../../HelperFiles/train_models.py?line=58'>59</a>\u001b[0m         loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> <a href='file:///Users/jeremygoldwasser/Desktop/RankSHAP/Experiments/Notebooks/../../HelperFiles/train_models.py?line=59'>60</a>\u001b[0m         optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     <a href='file:///Users/jeremygoldwasser/Desktop/RankSHAP/Experiments/Notebooks/../../HelperFiles/train_models.py?line=60'>61</a>\u001b[0m     \u001b[39m# print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/jeremygoldwasser/Desktop/RankSHAP/Experiments/Notebooks/../../HelperFiles/train_models.py?line=62'>63</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mneural_net\u001b[39m(x):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/shap/lib/python3.9/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/jeremygoldwasser/opt/anaconda3/envs/shap/lib/python3.9/site-packages/torch/optim/optimizer.py?line=85'>86</a>\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m     <a href='file:///Users/jeremygoldwasser/opt/anaconda3/envs/shap/lib/python3.9/site-packages/torch/optim/optimizer.py?line=86'>87</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> <a href='file:///Users/jeremygoldwasser/opt/anaconda3/envs/shap/lib/python3.9/site-packages/torch/optim/optimizer.py?line=87'>88</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/shap/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/jeremygoldwasser/opt/anaconda3/envs/shap/lib/python3.9/site-packages/torch/autograd/grad_mode.py?line=23'>24</a>\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     <a href='file:///Users/jeremygoldwasser/opt/anaconda3/envs/shap/lib/python3.9/site-packages/torch/autograd/grad_mode.py?line=24'>25</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     <a href='file:///Users/jeremygoldwasser/opt/anaconda3/envs/shap/lib/python3.9/site-packages/torch/autograd/grad_mode.py?line=25'>26</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> <a href='file:///Users/jeremygoldwasser/opt/anaconda3/envs/shap/lib/python3.9/site-packages/torch/autograd/grad_mode.py?line=26'>27</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/shap/lib/python3.9/site-packages/torch/autograd/grad_mode.py:131\u001b[0m, in \u001b[0;36mno_grad.__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/jeremygoldwasser/opt/anaconda3/envs/shap/lib/python3.9/site-packages/torch/autograd/grad_mode.py?line=129'>130</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__exit__\u001b[39m(\u001b[39mself\u001b[39m, exc_type: Any, exc_value: Any, traceback: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/jeremygoldwasser/opt/anaconda3/envs/shap/lib/python3.9/site-packages/torch/autograd/grad_mode.py?line=130'>131</a>\u001b[0m     torch\u001b[39m.\u001b[39;49mset_grad_enabled(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprev)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/shap/lib/python3.9/site-packages/torch/autograd/grad_mode.py:217\u001b[0m, in \u001b[0;36mset_grad_enabled.__init__\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/jeremygoldwasser/opt/anaconda3/envs/shap/lib/python3.9/site-packages/torch/autograd/grad_mode.py?line=214'>215</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, mode: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/jeremygoldwasser/opt/anaconda3/envs/shap/lib/python3.9/site-packages/torch/autograd/grad_mode.py?line=215'>216</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprev \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mis_grad_enabled()\n\u001b[0;32m--> <a href='file:///Users/jeremygoldwasser/opt/anaconda3/envs/shap/lib/python3.9/site-packages/torch/autograd/grad_mode.py?line=216'>217</a>\u001b[0m     torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_set_grad_enabled(mode)\n\u001b[1;32m    <a href='file:///Users/jeremygoldwasser/opt/anaconda3/envs/shap/lib/python3.9/site-packages/torch/autograd/grad_mode.py?line=217'>218</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m=\u001b[39m mode\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test, mapping_dict = load_data(\"census\", \"../Data\")\n",
    "xloc = X_test[0:1]\n",
    "# model, approx, true_shap_vals = train_logreg(X_train, y_train, xloc, mapping_dict)\n",
    "model, approx, true_shap_vals = train_neural_net(X_train, y_train, xloc, mapping_dict)\n",
    "\n",
    "print(\"Class imbalance: {}%\".format(round(100*(max(np.mean(y_test), 1-np.mean(y_test))))))\n",
    "Y_preds = (model(X_test) > 0.5).astype(\"int\")\n",
    "print(\"NN {}% accuracy\".format(round(np.mean(Y_preds == y_test)*100)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary run of Shapley sampling\n",
    "\n",
    "Not infrequently, expected number of samples to be significant exceeds max number of perms; sampling it, though, it's significant. This suggests our distribution is too conservative. Maybe we can remove the factor of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# %run shapley_sampling2\u001b[39;00m\n\u001b[1;32m      2\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m6\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m shap_vals, diffs_all_feats \u001b[38;5;241m=\u001b[39m shapley_sampling_adaptive(\u001b[43mmodel\u001b[49m, X_train, xloc, K\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, mapping_dict\u001b[38;5;241m=\u001b[39mmapping_dict, multiplicative\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_n_perms\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# %run shapley_sampling2\n",
    "np.random.seed(6)\n",
    "\n",
    "shap_vals, diffs_all_feats = shapley_sampling_adaptive(model, X_train, xloc, K=1, alpha=0.2, mapping_dict=mapping_dict, multiplicative=True, max_n_perms=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.587991810859778 0.66466135\n",
      "[0.24051316 0.13317972 0.12231014 0.05500751 0.04828939 0.03875463\n",
      " 0.01877386 0.01469499 0.00530414 0.00346821 0.00197111 0.0007789 ]\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(shap_vals), model(xloc) - np.mean(model(X_train)))\n",
    "\n",
    "print(np.sort(np.abs(shap_vals))[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate Experimental FWER\n",
    "\n",
    "## Model itself\n",
    "\n",
    "Recommended number of samples often exceeds max # perms by a LOT, but then running the max # usually works. Makes me question whether the boundary is valid.\n",
    "\n",
    "Using n_init=30 makes FWER more interesting, so that's what we show. Often n_init=50 makes the FWER 0. \n",
    "\n",
    "Still not sure one way or the other whether we need the factor of 2. Without, it's the Welch's t-test statistic. All these results are with the factor of 2.\n",
    "\n",
    "results in 11/9 table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "%run shapley_sampling2\n",
    "# np.random.seed(2)\n",
    "K = 1\n",
    "top_K = []\n",
    "for i in range(100):\n",
    "    # print(i)\n",
    "    shap_vals2, diffs_all_feats2 = shapley_sampling_adaptive(model, X_train, xloc, K=K, alpha=0.2, \n",
    "                            mapping_dict=mapping_dict, max_n_perms=10000, n_init=30, scale_var=True,\n",
    "                            multiplicative=False)\n",
    "    if shap_vals2 != \"NA\":\n",
    "        est_top_K = get_ordering(shap_vals2)[:K]\n",
    "        top_K.append(est_top_K)\n",
    "        print(len(top_K))\n",
    "        # if (i+1) % 5==0: \n",
    "        #     print(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "[30, 30, 30, 92, 30, 30, 30, 30, 92, 30, 30, 30]\n",
      "100 99\n"
     ]
    }
   ],
   "source": [
    "top_K = np.array(top_K)\n",
    "most_common_row = mode_rows(top_K)\n",
    "ct = 0\n",
    "for idx in range(top_K.shape[0]):\n",
    "    if np.array_equal(most_common_row, top_K[idx]):\n",
    "        ct += 1\n",
    "print(np.round(1 - ct / top_K.shape[0], 2)) # \"FWER\n",
    "print([len(diffs_all_feats2[j]) for j in range(len(diffs_all_feats2))])\n",
    "print(top_K.shape[0], i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic approximation\n",
    "- GLM, Credit, BRCA, K=5: controls FWER\n",
    "- NN, Credit, K=2: doesn't converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run shapley_sampling2\n",
    "# np.random.seed(1)\n",
    "# K = 5\n",
    "# true_order = np.argsort(np.abs(true_shap_vals))[::-1]\n",
    "# true_top_K = true_order[:K]\n",
    "# same_top_K = []\n",
    "# for i in range(100):\n",
    "#     shap_vals2, diffs_all_feats2 = shapley_sampling_adaptive(approx, X_train, xloc, K=K,\n",
    "#                                                 alpha=0.2, mapping_dict=mapping_dict)\n",
    "#     est_top_K = np.argsort(np.abs(shap_vals2))[::-1][:K]\n",
    "#     has_same_top_K = np.array_equal(true_top_K, est_top_K)\n",
    "#     same_top_K.append(has_same_top_K)\n",
    "#     if (i+1) % 5==0: print(i+1, np.round(1-np.mean(same_top_K),2)) # FWER\n",
    "# np.round(1-np.mean(same_top_K),2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "768407c71a286f507fab4bce553d71b5cbd766c247b76eb598ef769225202bc3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.16 ('shap')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
